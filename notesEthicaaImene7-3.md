Annotations
(19/02/2026 13:25:15)
« L’éthique nous invite à intégrer le fait que ce qui est technologiquement possible n’est pas toujours humainement ou socialement souhaitable. » (“Ethique et agents autonomes”, p. 5) (pdf) Je suis entièrement d’accord avec cette idée. Le fait qu’une chose soit technologiquement possible ne signifie pas qu’elle soit moralement acceptable ou socialement bénéfique. L’éthique nous rappelle que le progrès doit avant tout servir l’humain et respecter des valeurs essentielles.
« l’éthique est réflexive au sens où elle nous invite à engager une réflexion sur le sens de nos actions » (“Ethique et agents autonomes”, p. 6) (pdf) Je suis entièrement d’accord avec cette définition. Elle souligne que l’éthique ne se limite pas à appliquer des règles, mais implique une réflexion critique sur nos choix et leurs conséquences, ce qui est essentiel dans le contexte des agents autonomes.
« si nous suivons par exemple l’impératif catégorique de Kant » (“Ethique et agents autonomes”, p. 6) (pdf) La référence à Kant est fondamentale, car elle permet d’ancrer la distinction entre morale et éthique dans une tradition philosophique solide. Elle donne un cadre théorique rigoureux à l’analyse proposée.
« la morale est universelle, elle commande de façon inconditionnelle (si nous suivons par exemple l’impératif catégorique de Kant) et s’impose à ce titre, ou devrait s’imposer, à tous. » (“Ethique et agents autonomes”, p. 6) (pdf) Je ne suis pas totalement d’accord avec cette idée, car les systèmes moraux varient selon les cultures, les époques et les contextes sociaux. La prétendue universalité morale peut donc être discutée.
« ce n’est pas parce que des concepts éthiques sont modélisés et mis en œuvre dans une application que les usages de cette dernière seront éthiques par construction » (“Ethique et agents autonomes”, p. 7) (pdf) Je suis totalement d’accord avec cette affirmation. La présence de principes éthiques dans un système ne garantit pas que ses usages réels seront éthiques. Les pratiques concrètes et les contextes d’utilisation jouent un rôle déterminant.
« il ne faut pas confondre les représentations de l’éthique (et les discours de légitimation qu’elle génère) et la pratique de l’éthique elle-même » (“Ethique et agents autonomes”, p. 7) (pdf) Ce passage est central car il distingue clairement la modélisation théorique de l’éthique et son application concrète. C’est un point fondamental pour comprendre les limites des approches formelles en intelligence artificielle.
« Il faut s’interroger sur ce qu’est une « valeur » ou un « cadre éthique » codé dans une machine : il s’agit de fait d’un élément de connaissance, mis sous une forme mathématique calculable, et dont la portée et le contenu sémantique sont très restrictifs par rapport à ce qu’on entend en philosophie par valeur ou cadre éthique » (“Ethique et agents autonomes”, p. 9) (pdf) Je suis totalement d’accord avec ce passage. Il rappelle avec justesse que traduire des valeurs éthiques en langage mathématique pour des machines simplifie nécessairement leur sens et leur portée. Cela souligne la limite essentielle des modélisations éthiques : elles peuvent guider les systèmes, mais ne remplacent jamais la réflexion humaine et la complexité réelle de l’éthique.
« De plus, il n’y a pas de risque technologique en soi mais des contextes (sociaux, économiques ou politiques) qui favorisent, ou non, une appropriation clairvoyante des objets technologiques » (“Ethique et agents autonomes”, p. 9) (pdf) Cette idée est intéressante, mais elle soulève des questions. Peut-on vraiment dire qu’il n’existe aucun risque technologique en soi ? Certaines technologies peuvent être intrinsèquement dangereuses (par exemple, l’IA autonome dans des contextes militaires). Le rôle du contexte est certes crucial, mais je me demande si cela suffit à neutraliser le risque inhérent à la technologie elle-même.
« Un même risque concerne l’éthique elle-même. Sa formalisation textuelle peut vite trahir la réflexivité dont elle est porteuse. » (“Ethique et agents autonomes”, p. 10) (pdf) Je suis entièrement d’accord avec cette observation. Formaliser l’éthique en texte ou en code peut réduire sa richesse réflexive et sa capacité à questionner les situations de manière contextuelle. Cela rappelle que la modélisation n’est qu’un outil et ne peut jamais remplacer la réflexion humaine.
« En second lieu, une arme n’est pas forcément létale – et une arme létale, quel que soit son niveau d’automatisation, n’est en aucun cas animée d’une intention de tuer » (“Ethique et agents autonomes”, p. 10) (pdf) Je suis d’accord avec cette précision importante : une arme n’a pas de volonté propre et ne peut pas « décider » de tuer. Cela rappelle que l’intention morale appartient toujours à l’humain qui conçoit ou utilise la technologie, et que l’automatisation ne transfère pas cette responsabilité à la machine.
« Du point de vue éthique, au-delà de la question de la délégation à une machine de la « décision » de vie ou de mort, de telles capacités supposeraient d’identifier automatiquement et de manière fine une situation, et d’évaluer si les actions envisagées respectent les principes d’humanité (éviter les maux superflus) » (“Ethique et agents autonomes”, p. 10) (pdf) Cette affirmation soulève un vrai questionnement. Peut-on réellement attendre d’une machine qu’elle évalue de façon fine et contextuelle ce qui respecte les principes d’humanité ? Même avec des algorithmes sophistiqués, le jugement moral et l’appréciation des « maux superflus » restent très difficiles à formaliser. Cela montre les limites actuelles de la délégation éthique à l’IA.
« Pour les Anciens, les termes morale (mores en latin) et éthique (ethos en grec) signifiaient la même chose et étaient la traduction l’un de l’autre. L’usage les distingue, voire les oppose, aujourd’hui. » (“Ethique et agents autonomes”, p. 11) (pdf) Ce passage est une référence clé pour comprendre l’origine et la distinction conceptuelle entre morale et éthique. Il fournit le contexte historique nécessaire pour saisir pourquoi ces termes ont évolué différemment et comment cela influence la réflexion éthique contemporaine.
« Les formules impératives générales, selon Ricœur, « ne deviennent des maximes concrètes d'action que reprises, retravaillées, ré-articulées dans des éthiques régionales, spéciales, telles éthique médicale, éthique judiciaire, éthique des affaires8 ». » (“Ethique et agents autonomes”, p. 12) (pdf) Les formules impératives générales, selon Ricœur, « ne deviennent des maximes concrètes d'action que reprises, retravaillées, ré-articulées dans des éthiques régionales, spéciales, telles éthique médicale, éthique judiciaire, éthique des affaires8
« En effet, il est possible d’être responsable dans un sens autre que moral sans être coupable alors que la culpabilité implique toujours la responsabilité morale » (“Ethique et agents autonomes”, p. 12) (pdf) Je suis tout à fait d’accord avec cette distinction subtile mais importante. Elle permet de différencier la responsabilité en termes de conséquences ou d’actions d’une responsabilité morale liée au jugement éthique. Cela éclaire bien les discussions sur la responsabilité des agents autonomes et des utilisateurs humains.
« La société doit donc inculquer à chaque personne les règles de comportement à l’égard des autres, et les faire respecter au moyen des lois, de la force publique et de la pression sociale. » (“Ethique et agents autonomes”, p. 13) (pdf) Je ne suis pas totalement d’accord avec cette affirmation. La société peut guider et éduquer, mais la moralité et le respect des autres ne peuvent pas être imposés uniquement par la loi ou la pression sociale. Cela réduit la responsabilité individuelle et la réflexion éthique personnelle, qui sont essentielles pour des choix vraiment responsables.
« Pour Spinoza, le libre arbitre n’existe pas. Si les humains se figurent être libres, c’est parce qu’ils ont conscience de leur désir mais ignorent tout des causes qui leur font désirer. C’est la nécessité qui s’impose à nous. Pour autant, cela n’implique pas qu’aucun changement ne soit possible puisque la raison, qui est en nous, est libre. Elle est libre, non pas dans le sens où elle aurait le choix, mais parce que sa nécessité propre est la marque de son indépendance. C’est l’indépendance de la vérité, c’est la libre nécessité du vrai dirait Spinoza. » (“Ethique et agents autonomes”, p. 13) (pdf) Ce passage est une référence philosophique majeure. La pensée de Spinoza sur le libre arbitre et la liberté de la raison apporte un cadre pour réfléchir à la responsabilité et à l’autonomie, que ce soit chez l’humain ou dans le contexte des agents autonomes. Elle montre que la liberté peut être comprise comme autonomie rationnelle plutôt que simple choix arbitraire.
« Finalement, l’agent autonome libre est celui qui se possède par la réflexion, celui qui compare, analyse, prévoit et juge les différentes séries de phénomènes » (“Ethique et agents autonomes”, p. 14) (pdf) Je suis entièrement d’accord. Cela souligne que la liberté d’un agent, qu’il soit humain ou autonome, réside dans sa capacité à réfléchir et à évaluer ses actions, et non dans un simple mécanisme automatique.
« Enfin, il est courant d’utiliser l’adjectif autonome pour qualifier une entité technique qui ne fait pas intervenir d’agent humain entre la prise d’information et l’action » (“Ethique et agents autonomes”, p. 14) (pdf) Cette définition soulève des questions : qualifier une machine d’« autonome » uniquement sur la base de l’absence d’intervention humaine peut être réducteur, car cela ne prend pas en compte la capacité de jugement, de réflexion ou de responsabilité associée à l’autonomie réelle.
« Jugement Un point important, et malheureusement trop souvent omis, porte sur le jugement entendu au sens classique de l’opération de connaissance qui fait passer de sensations ou de perceptions à un concept. Un individu qui aurait perdu l’usage de ses sens ou serait victime d’hallucinations ne serait pas nécessairement en mesure de porter un jugement sur une situation » (“Ethique et agents autonomes”, p. 14) (pdf) Je suis totalement d’accord. Ce passage rappelle l’importance des capacités cognitives et perceptives pour exercer un jugement réfléchi. Cela montre aussi les limites des systèmes automatisés qui n’ont pas la richesse sensorielle et conceptuelle d’un humain.
« Le jugement est à l’origine de tout comportement rationnel ; sans lui, il n’y a ni liberté, ni éthique possible. » (“Ethique et agents autonomes”, p. 15) (pdf) Je suis entièrement d’accord. Le jugement est fondamental pour agir de manière rationnelle et éthique. Sans cette capacité à évaluer et décider, la notion de liberté et de responsabilité morale perd tout sens.
« Délibération Après avoir identifié la situation, l’agent autonome détermine les actions à accomplir au cours d’une phase critique de délibération, soit individuelle, soit collective, que l’on met en œuvre au travers de modèles informatiques. » (“Ethique et agents autonomes”, p. 15) (pdf) Cette idée soulève une interrogation : peut-on vraiment assimiler la délibération humaine, avec toutes ses nuances et réflexions contextuelles, à une phase calculée par des modèles informatiques ? Les limites de la formalisation méritent d’être questionnées.
« Dans le cas de la délibération individuelle, il convient aussi de confronter toutes les options en déployant les arguments en faveur ou en défaveur de chacune d’entre elles. » (“Ethique et agents autonomes”, p. 15) (pdf) Je suis d’accord. Cette description correspond bien au processus classique de délibération : examiner les options et peser les arguments pour prendre une décision réfléchie, ce qui est central pour l’éthique et la rationalité.
« ce qui correspond à la sagacité, ou en termes kantiens à l’impératif problématique, autrement dit à la capacité à découvrir les actions à accomplir les plus appropriées pour réaliser les objectifs que l’on s’est fixés » (“Ethique et agents autonomes”, p. 16) (pdf) Cette référence à Kant est essentielle car elle relie la notion de sagacité à l’impératif problématique, soulignant que la délibération implique de déterminer les actions les plus appropriées dans un contexte donné, et non de suivre aveuglément des règles fixes.
« ce qui relève de la moralité, à savoir en termes kantiens l’impératif moral, qui assure que les actions n’enfreignent pas les maximes de la propre volonté de l’agent, autrement dit les règles que l’on s’est données et qui se présentent comme des prescriptions morales. » (“Ethique et agents autonomes”, p. 16) (pdf) Cette référence à l’impératif moral de Kant est cruciale. Elle souligne que la moralité repose sur le respect des règles que l’agent s’est librement données, ce qui établit un lien direct entre autonomie, responsabilité et éthique.
« Pour mettre en lumière ces conflits, nous recourons à des formalismes logiques développés en Intelligence Artificielle mettant en œuvre des raisonnements « non monotones » qui permettent de faire face à des contradictions. » (“Ethique et agents autonomes”, p. 16) (pdf) L’idée est intéressante, mais je me questionne sur la capacité réelle des formalismes logiques à gérer toutes les contradictions éthiques complexes. La modélisation informatique peut aider, mais peut-elle vraiment refléter la nuance et la flexibilité du raisonnement humain ?
« Il convient de remarquer qu’un gouffre important existe à ce jour entre normes juridiques (très générales), normes techniques (très précises) et valeurs (difficiles à hiérarchiser) » (“Ethique et agents autonomes”, p. 17) (pdf) Je suis entièrement d’accord. Ce passage souligne avec justesse le décalage entre les différents types de règles et valeurs. Comprendre ce gouffre est essentiel pour développer des systèmes autonomes capables d’agir de manière éthique et conforme aux normes.
« L’axiologie et le traitement des valeurs doivent déboucher sur des définitions formelles explicites ou des taxonomies utilisables en Intelligence Artificielle. » (“Ethique et agents autonomes”, p. 17) (pdf) Cette exigence soulève un doute : transformer les valeurs en définitions formelles ou taxonomies peut simplifier excessivement leur complexité. Comment s’assurer que ces représentations conservent toute la richesse et la nuance des valeurs éthiques humaines ?
« De plus, si l’on se place dans le cadre d’un système opérateur-robot, il est intéressant de pouvoir signaler à l’opérateur par la levée d’une alarme que l’agent autonome rencontre une situation nécessitant son intervention. » (“Ethique et agents autonomes”, p. 19) (pdf) Je suis d’accord. Prévoir une alerte à l’opérateur en cas de situation complexe est une mesure pragmatique qui combine autonomie et supervision humaine, ce qui renforce la sécurité et la responsabilité dans l’usage des agents autonomes.
« Il s’agit de permettre à un agent de tenir compte de la pluralité des valeurs et principes des autres agents. » (“Ethique et agents autonomes”, p. 19) (pdf) Je suis entièrement d’accord. Prendre en compte la pluralité des valeurs et principes est crucial pour que les agents autonomes puissent interagir de manière éthique dans des environnements multi-agents, reflétant la diversité des préférences et des normes.
« Cependant, dans le cadre d’un agent ayant à mener un raisonnement sur les décisions, nous estimons qu’il n’est pas pertinent de considérer cette définition. » (“Ethique et agents autonomes”, p. 20) (pdf) Cette remarque soulève un point intéressant : on peut se demander pourquoi certaines définitions ou approches ne sont pas jugées pertinentes dans un contexte donné. Cela invite à réfléchir sur les critères qui rendent une définition ou un principe réellement utile dans la prise de décision.
« Lorsqu’une décision a un impact sur le monde, il est difficile voire impossible d’en appréhender toutes les conséquences. » (“Ethique et agents autonomes”, p. 20) (pdf) Je suis totalement d’accord. Toute décision comporte une part d’incertitude et il est rarement possible d’en prévoir toutes les conséquences. Cela souligne l’importance de la prudence et de la réflexion avant d’agir.
« Un agent a des capacités de perception limitées, et de plus ces perceptions varient d’un agent à l’autre. Non seulement un agent pourra ne pas prendre en compte une conséquence qu’un autre agent considère, mais en outre il pourra évaluer cette conséquence différemment. Ainsi, une situation pourra être un dilemme pour un agent mais pas pour un autre. » (“Ethique et agents autonomes”, p. 21) (pdf) Je suis entièrement d’accord. Ce passage montre bien que la perception et l’évaluation sont subjectives. Ce qui constitue un dilemme pour un individu peut ne pas l’être pour un autre, ce qui souligne l’importance de considérer différents points de vue dans la prise de décision.
« Une distinction se fait entre une causalité générale (« Rouler vite provoque des accidents ») et une causalité effective (« Le fait que Caitlyn ait roulé vite a provoqué son accident d’aujourd’hui ») » (“Ethique et agents autonomes”, p. 21) (pdf) Je suis totalement d’accord. Cette distinction entre causalité générale et effective est très pertinente pour comprendre comment les principes théoriques se traduisent dans des situations concrètes, ce qui est essentiel pour évaluer les conséquences d’une action.
« Suivant le philosophe anglais David Hume, une approche courante est de définir la causalité en termes de dépendance contre-factuelle : A est une cause de B quand, si A n’était pas arrivé, B ne serait pas arrivé non plus » (“Ethique et agents autonomes”, p. 21) (pdf) La référence à Hume est essentielle ici. Elle fournit un cadre philosophique classique pour comprendre la causalité, ce qui est crucial pour analyser les conséquences des actions et structurer la réflexion sur les décisions et leurs effets.
« Les modélisations et mises en œuvre réalisées dans le cadre d’ETHICAA ont d’abord pour objectif de juger une décision d'un point de vue éthique ou axiologique, soit dans l'absolu, soit par rapport à d'autres décisions possibles. » (“Ethique et agents autonomes”, p. 22) (pdf) Je suis entièrement d’accord. L’objectif de ces modélisations est clair : elles visent à évaluer les décisions selon des critères éthiques, ce qui permet de comparer différentes options et de réfléchir aux choix les plus appropriés.
« Un modèle du bien qui donne une appréciation de la valeur morale intrinsèque de finalités ou d’événements. Parmi les modalités du bien, on trouve par exemple les vertus de l’agent décideur (altruisme, courage...), le droit des agents qui subissent les conséquences des décisions (droit à la vie, droit à la propriété...), le bien-être de tous les agents. » (“Ethique et agents autonomes”, p. 23) (pdf) Je suis d’accord. Mettre en avant un modèle du bien qui considère à la fois les vertus de l’agent, les droits des autres et le bien-être collectif permet une évaluation éthique complète et équilibrée des décisions.
« L’éthique pour la décision a été abordée en s’intéressant d’une part à un modèle d’action et de causalité, et d’autre part à un modèle de jugement individuel. » (“Ethique et agents autonomes”, p. 24) (pdf) Commentaire :  
Cette approche soulève une question : se concentrer sur un cadre mono-agent peut-il vraiment capturer toutes les implications éthiques d’une décision, surtout dans des contextes où les interactions avec d’autres agents ou humains sont importantes ?
« Un modèle de jugement éthique a été intégré au sein d’une architecture réflexive d’agent de type « Belief/Desire/Intention ». S'appuyant sur une représentation locale et à jour de la situation courante perçue par l'agent, ce modèle permet de réaliser un jugement de l'éthique en contexte. » (“Ethique et agents autonomes”, p. 24) (pdf) Je suis d’accord. Intégrer un modèle de jugement éthique dans l’architecture BDI permet à l’agent de prendre des décisions contextualisées et réfléchies, ce qui reflète bien la dimension pratique et situationnelle de l’éthique.
« Plus généralement, l’usage quasiment systématique du profilage des identités exclut le changement de cap et le décentrement, éliminant, de ce fait, la possibilité d’explorer différents modes d’action. Dans le cadre du projet ETHICAA, nous nous sommes centrés sur la problématisation et la modélisation de conflits éthiques, sans travailler directement sur les questions d'apprentissage et de profilage » (“Ethique et agents autonomes”, p. 25) (pdf) Cette remarque soulève une interrogation : en excluant l’apprentissage et le profilage, peut-on vraiment représenter toutes les dimensions des décisions éthiques des agents autonomes ? Le choix de se concentrer sur les conflits éthiques est pertinent, mais limite peut-être la vision complète des comportements possibles.
« Selon le degré de connaissance de l'« agent juge » sur les théories du bien et du juste de l'« agent jugé », plusieurs types de jugements peuvent être mis en œuvre : 1. jugement aveugle (l'agent juge utilise ses propres théories uniquement), 2. partiellement informé (l’agent juge se fonde sur une connaissance partielle des théories de l’agent jugé), 3. totalement informé (l'agent juge se fonde sur une connaissance totale). » (“Ethique et agents autonomes”, p. 26) (pdf) Je suis entièrement d’accord. Cette classification est claire et pertinente : elle montre comment le niveau de connaissance de l’agent juge influence la qualité et la précision du jugement éthique, ce qui est crucial pour évaluer correctement les décisions.
« Formation éthique de collectifs Dans le cadre de la formation éthique de collectifs d’agents, c’està-dire de groupes d’agents qui vont devoir par la suite collaborer pour effectuer une action, nous avons proposé une modélisation d’une éthique du bien fondée sur les vertus » (“Ethique et agents autonomes”, p. 27) (pdf) Je suis d’accord. Introduire une éthique fondée sur les vertus pour des collectifs d’agents permet de guider la collaboration de manière cohérente et responsable, en prenant en compte à la fois les actions individuelles et l’efficacité du groupe.
« Du besoin de justification Une explication décrit comment une décision donnée a été calculée. Il s’agit alors d’une représentation compacte d’une trace d’exécution mettant en lumière les principaux éléments qui ont conduit à la prise de décision » (“Ethique et agents autonomes”, p. 27) (pdf) Fournir une justification claire pour chaque décision est essentiel, car cela permet de comprendre le raisonnement derrière l’action et de renforcer la transparence et la responsabilité.
« Cependant, vouloir produire des explications au regard de critères éthiques pose de nouvelles questions. En effet, comme l’éthique est un raisonnement en contexte, les 27 explications doivent tenir compte de cette notion. Il ne s’agit plus seulement d'expliquer les décisions mais aussi d’expliquer la manière dont un agent évalue la situation dans laquelle il prend une décision. C’est pour cela qu’il est préférable de considérer des justifications. » (“Ethique et agents autonomes”, p. 27) (pdf) Cette remarque soulève une interrogation importante : comment rendre compte de manière compréhensible de l’évaluation contextuelle d’un agent ? Les justifications sont utiles, mais il reste difficile de transmettre toute la complexité du raisonnement éthique dans une explication concise.
« Arguments pour représenter l’éthique Au cours du projet ETHICAA, nous avons ainsi proposé un modèle formel d’argumentation éthique, inspiré du modèle des ordres d’André Comte-Sponville13. Dans ce modèle, un agent à éthique artificielle est doté de plusieurs théories logiques : une théorie des croyances, une théorie des désirs, une théorie d’action, une théorie normative, une théorie morale et une théorie des valeurs. Chaque théorie permet, en fonction du contexte, de générer des arguments. Par exemple, la théorie normative va générer des arguments exprimant qu’il est permis de faire A dans le contexte C et la théorie morale qu’il est bon de 28 13 André Comte-Sponville. Le capitalisme estil moral ? Albin Michel. 2004. faire A’ dans le contexte C au nom d’une valeur V. Ces arguments interagissent en mélangeant une approche déontologique (certains arguments sont plus ou moins préférés a priori par l’agent) et conséquentialiste (plus il y a d’arguments en faveur d’une décision, plus cette dernière est acceptable). » (“Ethique et agents autonomes”, p. 28) (pdf) Cette référence à Comte-Sponville et la modélisation détaillée des différentes théories est essentielle. Elle montre comment les arguments éthiques peuvent être structurés et pondérés pour guider les décisions d’un agent, en combinant approches déontologiques et conséquentialistes.
« Dans le cadre du projet ETHICAA, il est apparu que le problème de savoir si un agent autonome vérifiait une certaine éthique pour l’utiliser dans un contexte donné était crucial. Cela est d’autant plus nécessaire que l’agent est a priori autonome, c’est-àdire que son comportement, défini par ses développeurs, peut être complexe et non maîtrisé » (“Ethique et agents autonomes”, p. 29) (pdf) Cette remarque soulève un vrai questionnement : comment peut-on réellement vérifier qu’un agent autonome respecte une éthique donnée si son comportement peut être complexe et imprévisible ? La vérification éthique semble difficile à garantir dans tous les contextes.
« Être intelligible et lisible par l’humain Si les systèmes techniques automatisés – dans le cas d’agents d’assistance à la personne par exemple peuvent contribuer au bien-être des personnes, une réflexion sur leurs conditions d’appropriation doit être ouvertement menée » (“Ethique et agents autonomes”, p. 32) (pdf) Même des systèmes bénéfiques doivent être conçus de manière à ce que leur utilisation soit compréhensible et appropriable par les humains, afin d’assurer un réel bien-être et une interaction éthique.
« Les réflexions de Norbert Wiener peuvent à cet égard constituer une source importante d’inspiration. Le père de la cybernétique ne manquait pas de souligner le risque de voir se développer une société qui ne serait plus à même de questionner le développement de ses propres inventions, le danger social de la machine ne tenant pas à la machine elle-même mais à l’usage que l’humain peut en faire ainsi qu’au risque d’absence de questionnement que la machine peut induire. C’est en ce sens et au regard de la complexité des interactions qui peuvent se nouer entre les humains et les agents autonomes que ces derniers doivent impérativement répondre à des exigences d’intelligibilité et de lisibilité. » (“Ethique et agents autonomes”, p. 32) (pdf) La référence à Norbert Wiener est essentielle. Elle rappelle que le danger des technologies réside souvent dans leur usage humain et dans le manque de réflexion critique de la société, soulignant l’importance de concevoir des agents intelligibles et transparents.
« Afin de proposer des mécanismes généraux et adaptables, introduire une modularité selon le niveau de généralité (les règles proposées sont-elles générales, spécifiques à un domaine ou à une situation ?) facilite la réutilisation des modèles dans d’autres applications ainsi que leur remplacement et leur adaptation. » (“Ethique et agents autonomes”, p. 33) (pdf) Je suis d’accord. La modularité est essentielle pour rendre les modèles éthiques flexibles et adaptables à différents contextes, ce qui permet leur réutilisation et leur évolution sans tout reconstruire à chaque fois.
« Le processus de jugement peut de plus être complété en articulant ces différents principes éthiques selon des préférences préétablies. Cette modularité assure l’indépendance entre les composants de l’architecture vis-à-vis de chacune des théories d’une part, et entre chacune des théories ellesmêmes d’autre part. » (“Ethique et agents autonomes”, p. 34) (pdf) Je suis entièrement d’accord. Cette approche modulaire et hiérarchisée permet de combiner plusieurs principes éthiques tout en conservant l’indépendance de chaque composant, ce qui renforce la cohérence et la flexibilité du jugement.
« De manière plus générale, il convient de se poser les questions suivantes : • un algorithme qui implique des considérations axiologiques ou éthiques doit-il être calqué sur les considérations axiologiques ou éthiques de l’humain, et si oui de quel humain ? Il faut en particulier remarquer que les attentes que l’on a vis-à-vis d’un algorithme peuvent être très différentes de celles qu’on a vis-à-vis d’un humain qui fournirait le « même » résultat ; • un être humain peut choisir de ne pas agir de façon morale : jusqu’où traduire cela dans un algorithme, programmer la dérogation aux règles, la transgression ? Il y a une indétermination de l’éthique qui semble incompatible avec une tentative de preuve ou de certification des algorithmes qui la mettraient en œuvre. De plus, il y a un paradoxe consistant à calquer le raisonnement humain, qui est faillible, dans un algorithme, et vouloir que cet algorithme soit infaillible. » (“Ethique et agents autonomes”, p. 35) (pdf) Ce passage soulève des questions essentielles sur les limites de la transposition de l’éthique humaine dans les algorithmes. Il invite à réfléchir sur la tension entre la complexité, la faillibilité humaine et l’exigence d’infaillibilité des systèmes automatisés, montrant que certaines dimensions éthiques sont difficiles à formaliser.
« Enfin, il y a certainement un danger à vouloir déléguer l’entendement à un algorithme, et de fonder des décisions uniquement sur le calcul. » (“Ethique et agents autonomes”, p. 35) (pdf) Je suis entièrement d’accord. Fonder des décisions uniquement sur le calcul risque d’éliminer la réflexion, le jugement et la responsabilité humaine, ce qui peut conduire à des choix déshumanisés ou éthiquement problématiques.
« Le suivi de patient à domicile La multiplication et la diversification d’objets connectés déployables dans des environnements domestiques ouvre des perspectives sur le suivi d’activités quotidiennes pour l’assistance ou la supervision » (“Ethique et agents autonomes”, p. 36) (pdf) Je suis d’accord. L’usage des objets connectés à domicile offre un réel potentiel pour le suivi et l’assistance des personnes, permettant d’améliorer la sécurité et le bien-être au quotidien.
« seul l’examen par les autres de notre éthique nous permet d’en évaluer la prétention à l’universalité et il ne peut y avoir d’éthique sans s’obliger à se placer du point de vue de tous les autres16. » (“Ethique et agents autonomes”, p. 37) (pdf) Je suis entièrement d’accord. L’éthique ne peut être réellement évaluée que par la confrontation avec les autres points de vue, ce qui garantit sa réflexion critique et sa prétention à l’universalité.
« L’appréhension du contexte est fondamentale pour apprécier une situation. L’exemple donné par Paul Scharre du Lieutenant Colonel Stanislav Petrov évitant une troisième guerre mondiale en est l’illustration parfaite17 » (“Ethique et agents autonomes”, p. 39) (pdf) La référence à l’exemple concret de Stanislav Petrov est essentielle. Elle illustre de manière frappante l’importance de comprendre le contexte pour prendre des décisions critiques et éthiquement responsables.
« Il est difficile de définir de manière exhaustive ce que contient le contexte. Cependant, nous pouvons constater que si « la situation ne porte pas en elle-même le jugement [éthique]19 » » (“Ethique et agents autonomes”, p. 39) (pdf) Cette remarque soulève une interrogation : si la situation seule ne contient pas le jugement éthique, comment s’assurer que l’évaluation faite par un agent ou un individu est complète et pertinente ? La définition et la prise en compte du contexte restent complexes et ouvertes.
« Il est aussi important de noter que le contexte embarque une notion d’évaluation du temps. En effet, le fait de devoir agir rapidement, ou d’avoir un impact à court, moyen ou long terme est à prendre en compte dans le contexte. » (“Ethique et agents autonomes”, p. 39) (pdf) Je suis d’accord. La dimension temporelle est cruciale pour évaluer correctement une situation et les conséquences des actions, car le timing influence directement la pertinence et l’éthique des décisions.
« Dans l’ensemble des travaux liés à la modélisation de l’éthique, le contexte est pratiquement toujours réduit à la situation. Cette simplification est un argument supplémentaire à la thèse soutenant qu’un système autonome ne peut avoir un raisonnement éthique à proprement parler, mais peut seulement embarquer des concepts éthiques dans son raisonnement. » (“Ethique et agents autonomes”, p. 40) (pdf) Cette analyse est pertinente : un système autonome ne peut pas vraiment « raisonner » éthiquement comme un humain, car il simplifie nécessairement le contexte. Il ne peut que manipuler des concepts éthiques intégrés dans son fonctionnement.
« Selon les domaines d’applications et plus particulièrement lorsque des agents autonomes agissent dans le monde physique, la dimension temporelle doit être prise en compte. Par exemple, les véhicules autonomes doivent être capables de décider de leurs actions en temps réel. » (“Ethique et agents autonomes”, p. 40) (pdf) Je suis d’accord. La contrainte de temps est essentielle : dans des situations réelles comme pour les véhicules autonomes, les décisions doivent être rapides, ce qui impose de concilier rapidité et réflexion éthique.
« La certification de produit ou de système consiste en une attestation délivrée par une tierce partie indépendante et impartiale que le produit ou le système répond, à un instant donné, à des exigences prédéfinies, répertoriées dans un référentiel20. » (“Ethique et agents autonomes”, p. 41) (pdf) La certification par une tierce partie apporte une garantie objective que le système respecte certaines exigences, ce qui est important pour la confiance et la sécurité, même si cela ne couvre pas tous les aspects éthiques.
« Cette question de la certification et des problématiques connexes est mise en avant dans les nombreux rapports et plans de recherche nationaux et internationaux parus ces dernières années, en lien avec le développement de systèmes s'appuyant sur des technologies de l’Intelligence Artificielle. » (“Ethique et agents autonomes”, p. 41) (pdf) Ce passage est important car il montre que la certification des systèmes autonomes est un enjeu reconnu au niveau national et international, soulignant l’intérêt croissant pour encadrer éthiquement le développement technologique.
« L'éthique d'un comportement ne peut se concevoir qu'en contexte » (“Ethique et agents autonomes”, p. 43) (pdf) L’éthique ne peut pas être évaluée de manière abstraite : elle dépend toujours des circonstances, du contexte et des interactions spécifiques dans lesquelles une action se produit.
« Il est donc nécessaire de consigner les situations caractéristiques dans lesquelles les exigences éthiques référencées doivent pouvoir être satisfaites, et quelle sémantique leur donner, car une même exigence éthique peut avoir des interprétations différentes selon le contexte. » (“Ethique et agents autonomes”, p. 43) (pdf) Consigner les situations et définir clairement leur sémantique est essentiel pour que les exigences éthiques soient comprises et appliquées correctement, car leur interprétation varie selon le contexte.
« L'éthique des comportements mis en œuvre par ces systèmes doit en effet pouvoir être certifiée vis-à-vis de telles 43 Qualifier de manière reproductible, répétable et interprétable les solutions et systèmes artificiels éthiques. 23 http://standards.ieee.org/news/2017/ ead_v2.html exigences » (“Ethique et agents autonomes”, p. 43) (pdf) Pour garantir la confiance et la fiabilité, il est crucial que les comportements éthiques des systèmes soient évaluables de manière reproductible, répétable et interprétable, conformément aux exigences définies.
« Auditer, tester et vérifier que ces systèmes artificiels satisfont aux critères. Tout comme pour les systèmes d’Intelligence Artificielle, l’estimation de la qualité de l’éthique de systèmes d’agents autonomes devra pouvoir s’effectuer à l’aide de protocoles de mesure dans un cadre commun fondé sur la reproductibilité, la répétabilité, et l’estimation de la justesse. Il s’agit de comparer des approches dans les mêmes conditions sur une tâche précise via des tests et des expérimentations, des simulations. Il s’agit de définir des cadres permettant de qualifier de manière reproductible, répétable et interprétable les solutions et systèmes artificiels éthiques. » (“Ethique et agents autonomes”, p. 44) (pdf) Cette approche soulève une interrogation : est-il vraiment possible de mesurer « la qualité de l’éthique » d’un système avec des protocoles reproductibles et des tests objectifs ? L’éthique comporte une dimension contextuelle et subjective qui peut échapper à toute formalisation ou expérimentation standardisée.
« Mais le défi éthique est entier dans une époque hypermoderne où nous sommes, d’une part, dans une relation plutôt consumériste aux nouveautés technologiques (tout ce qui est nouveau est de ce fait massivement considéré comme allant dans le sens du progrès) et où, d’autre part, la majorité des usagers est loin de disposer de clés de compréhension nécessaires au déchiffrage de leurs environnements hyper-technologiques (objets connectés, agents autonomes, puces RFID, biométrie, etc.) qui apportent souvent dans la vie de tous les jours un plus grand confort. Or, comme l’exprimait déjà Herbert Marcuse, plus une certaine « commodité » nous est technologiquement garantie, plus elle se voit induite par la normalisation de nos pratiques technologiques, moins le champ des interrogations que nous serions en droit de formuler vis-à-vis de toute innovation technoscientifique (en termes de sens, de valeurs, de qualité du lien social, etc.) est intense et profond. Se donner pour tâche « d’orienter le présent vers un Aucune éthique ne peut s’élaborer indépendamment d’une discussion ouverte et contradictoire. avenir durable » suppose donc de travailler sans relâche à l’éveil d’une démarche critique constructive à l’ère hypermoderne. » (“Ethique et agents autonomes”, p. 47) (pdf) Cette analyse est pertinente, mais je me questionne : est-il réaliste d’espérer que tous les usagers acquièrent les clés de compréhension nécessaires dans un environnement hyper-technologique ? Le défi critique semble immense face à la complexité croissante des technologies et des pratiques.
« Aucune éthique ne peut s’élaborer indépendamment d’une discussion ouverte et contradictoire. » (“Ethique et agents autonomes”, p. 47) (pdf) Cette phrase souligne un principe fondamental : l’éthique se construit dans le dialogue et la confrontation des points de vue. Sans échanges critiques, il est impossible d’atteindre une réflexion éthique complète et partagée.
« Aucune éthique ne peut s’élaborer indépendamment d’une discussion ouverte et contradictoire. » (“Ethique et agents autonomes”, p. 47) (pdf)
« Aucune éthique ne peut s’élaborer indépendamment d’une discussion ouverte et contradictoire. » (“Ethique et agents autonomes”, p. 47) (pdf)
« Il convient de s’interroger sur le sens de l’agir humain, en tenant compte du fait que toute action échappe de plus en plus à la volonté de son auteur à mesure qu’elle s’inscrit dans le jeu de rétroactions de l’environnement où elle intervient. C’est ce qu’une écologie de l’action montre clairement : dans la pratique, l’intention risque le plus souvent de se traduire par un échec dans la mesure où les effets de l’action dépendent non seulement des intentions de celui qui agit, mais aussi des contextes où l’action se déroule. Or, c’est bel et bien la multitude (et la complexité) de ces interactions qu’une éthique des agents autonomes devra être en mesure d’embrasser. » (“Ethique et agents autonomes”, p. 48) (pdf) Je ne suis pas entièrement d’accord avec l’idée que « toute action échappe de plus en plus à la volonté de son auteur ». Bien que l’environnement influence les effets des actions, les individus gardent une part de responsabilité et de contrôle sur leurs choix ; réduire l’action humaine à un simple jeu de rétroactions me semble trop déterministe.
